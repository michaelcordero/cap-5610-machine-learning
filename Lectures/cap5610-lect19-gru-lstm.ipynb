{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GRU and LSTM\n* In this demo, we will apply the GRU and LSTM on the Sunspot dataset (https://www.kaggle.com/robervalt/sunspots)\n* See demo in Lecture 18 for details about dataset and pre-processing.","metadata":{}},{"cell_type":"markdown","source":"## 1. Read, pre-process, and split dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom pandas import read_csv\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Download and read dataset\nsunspots_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv'\ndf = read_csv(sunspots_url)\n\n# Pre-process features\nscaler = MinMaxScaler(feature_range=(0, 1))\ndata = np.array(df['Sunspots'], dtype='float32').reshape((-1, 1))\ndata = scaler.fit_transform(data).flatten()\n\n# Split dataset into train/test sets\nsplit_ratio = 0.8 # Percentage to use for training\n\nsplit_index = int(len(data)*split_ratio) # Index to split train/test sets\ntrain_data = data[:split_index]\ntest_data = data[split_index:]\n\nprint(train_data.shape, test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:05:21.163538Z","iopub.execute_input":"2022-03-24T23:05:21.163941Z","iopub.status.idle":"2022-03-24T23:05:22.540651Z","shell.execute_reply.started":"2022-03-24T23:05:21.163844Z","shell.execute_reply":"2022-03-24T23:05:22.539647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Prepare inputs and targets for each split","metadata":{}},{"cell_type":"code","source":"# Function to prepare the input X and target Y\ndef get_XY(dat, time_steps, sliding_steps):\n    # Prepare the targets Y\n    Y_ind = np.arange(time_steps, len(dat), sliding_steps)\n    Y = dat[Y_ind]\n    \n    # Prepare the inputs X\n    X = []\n    for i in Y_ind:\n        X.append(dat[i-time_steps:i])\n    X = np.array(X).reshape((len(Y), time_steps, 1))\n    \n    return X, Y\n\ntime_steps = 12\nsliding_steps = 12\ntrainX, trainY = get_XY(train_data, time_steps, sliding_steps)\ntestX, testY = get_XY(test_data, time_steps, sliding_steps)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:06:04.688321Z","iopub.execute_input":"2022-03-24T23:06:04.688947Z","iopub.status.idle":"2022-03-24T23:06:04.697464Z","shell.execute_reply.started":"2022-03-24T23:06:04.688892Z","shell.execute_reply":"2022-03-24T23:06:04.696576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Define, train, and test GRU model\n* We define an GRU model with 3 hidden nodes, then a Dense layer with 1 hidden node (for regression).\n* We train and evaluate the model using MSE loss.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, GRU\nfrom sklearn.metrics import mean_squared_error\n\n# Define model\nmodel_gru = Sequential()\nmodel_gru.add(GRU(units=3, input_shape=(time_steps, 1)))\nmodel_gru.add(Dense(units=1, activation='tanh'))\n\n# Compile and train model\nmodel_gru.compile(loss='mean_squared_error', optimizer='adam')\nmodel_gru.fit(trainX, trainY, epochs=20, batch_size=5)\n\n# Make prediction and compute test MSE\ntrain_predict_gru = model_gru.predict(trainX)\ntest_predict_gru = model_gru.predict(testX)\n\ntrain_mse = mean_squared_error(trainY, train_predict_gru)\ntest_mse = mean_squared_error(testY, test_predict_gru)\n\nprint('Train MSE: %.3f' % (train_mse))\nprint('Test MSE: %.3f' % (test_mse))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:07:15.023933Z","iopub.execute_input":"2022-03-24T23:07:15.024209Z","iopub.status.idle":"2022-03-24T23:07:33.085145Z","shell.execute_reply.started":"2022-03-24T23:07:15.024182Z","shell.execute_reply":"2022-03-24T23:07:33.084228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Define, train, and test LSTM model\n* We define an LSTM model with 3 hidden nodes, then a Dense layer with 1 hidden node (for regression).\n* We train and evaluate the model using MSE loss.","metadata":{}},{"cell_type":"code","source":"from keras.layers import LSTM\n\n# Define model\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(units=3, input_shape=(time_steps, 1)))\nmodel_lstm.add(Dense(units=1, activation='tanh'))\n\n# Compile and train model\nmodel_lstm.compile(loss='mean_squared_error', optimizer='adam')\nmodel_lstm.fit(trainX, trainY, epochs=40, batch_size=5)\n\n# Make prediction and compute test MSE\ntrain_predict_lstm = model_lstm.predict(trainX)\ntest_predict_lstm = model_lstm.predict(testX)\n\ntrain_mse = mean_squared_error(trainY, train_predict_lstm)\ntest_mse = mean_squared_error(testY, test_predict_lstm)\n\nprint('Train MSE: %.3f' % (train_mse))\nprint('Test MSE: %.3f' % (test_mse))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:12:17.758806Z","iopub.execute_input":"2022-03-24T23:12:17.75911Z","iopub.status.idle":"2022-03-24T23:12:32.864502Z","shell.execute_reply.started":"2022-03-24T23:12:17.759079Z","shell.execute_reply":"2022-03-24T23:12:32.863122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Plot the true targets and predictions","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntrue_targets = np.append(trainY, testY)\npredicted_targets_gru = np.append(train_predict_gru, test_predict_gru)\npredicted_targets_lstm = np.append(train_predict_lstm, test_predict_lstm)\n\nplt.figure(figsize=(15, 6), dpi=80)\nplt.plot(true_targets)\nplt.plot(predicted_targets_gru)\nplt.plot(predicted_targets_lstm)\nplt.axvline(x=len(trainY), color='r')\nplt.legend(['True target', 'GRU Prediction', 'LSTM Prediction'])\nplt.xlabel('Target index')\nplt.ylabel('Sunspots scaled')\nplt.title('Actual and predicted target values. The red line separates the training and test examples.')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:11:48.071064Z","iopub.execute_input":"2022-03-24T23:11:48.071645Z","iopub.status.idle":"2022-03-24T23:11:48.364517Z","shell.execute_reply.started":"2022-03-24T23:11:48.071529Z","shell.execute_reply":"2022-03-24T23:11:48.363709Z"},"trusted":true},"execution_count":null,"outputs":[]}]}