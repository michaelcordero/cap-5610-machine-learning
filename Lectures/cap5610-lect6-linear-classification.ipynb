{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# CAP5610 - Linear classification\n\nIn this demo, we will train and test a logistic regression model on a toy classification dataset.",
   "metadata": {
    "id": "63GSckRspoZJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### 1. Generate the toy dataset\n\nWe can use the [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function from sklearn to generate the data. The API for this function is at: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html.",
   "metadata": {
    "id": "h9MDqr8IxNuh"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.datasets import make_classification\n\nX, Y = make_classification(n_samples=200,\n                           n_features=2,\n                           n_informative=2,\n                           n_redundant=0,\n                           n_classes=2,\n                           random_state=10)\n\nprint(X.shape, Y.shape)",
   "metadata": {
    "id": "MefuasTHjBvI",
    "execution": {
     "iopub.status.busy": "2022-01-28T00:19:32.070732Z",
     "iopub.execute_input": "2022-01-28T00:19:32.071682Z",
     "iopub.status.idle": "2022-01-28T00:19:33.266186Z",
     "shell.execute_reply.started": "2022-01-28T00:19:32.071567Z",
     "shell.execute_reply": "2022-01-28T00:19:33.264976Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Visualize the dataset\n\nWe can visualize the dataset by plotting the points with different colors for each class.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\nplt.scatter(X[:,0], X[:,1], c=Y, cmap=plt.cm.coolwarm, s=50, edgecolors='k')\nplt.xlabel('x0')\nplt.ylabel('x1')\nplt.show()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-28T00:20:14.942004Z",
     "iopub.execute_input": "2022-01-28T00:20:14.942343Z",
     "iopub.status.idle": "2022-01-28T00:20:15.186044Z",
     "shell.execute_reply.started": "2022-01-28T00:20:14.942312Z",
     "shell.execute_reply": "2022-01-28T00:20:15.185189Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Split the data\n\nNow we split the data into a train set (60%) and a test set (40%).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n\nprint(X_train.shape)\nprint(X_test.shape)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-28T00:21:00.903655Z",
     "iopub.execute_input": "2022-01-28T00:21:00.904481Z",
     "iopub.status.idle": "2022-01-28T00:21:00.962056Z",
     "shell.execute_reply.started": "2022-01-28T00:21:00.904429Z",
     "shell.execute_reply": "2022-01-28T00:21:00.961294Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Define and train a logistic regression model on the train set\n\nWe define a logistc regression model with l2 regularization and C=1.0. Then we train the model on the train set. The API for logistic regression is at: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html.",
   "metadata": {
    "id": "v5_wyZRu6CF_"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(C=1.0) # Define the logistic regression model\nmodel.fit(X_train, Y_train) # Train the model on the train set",
   "metadata": {
    "id": "34kjAIGf6Ghn",
    "execution": {
     "iopub.status.busy": "2022-01-28T00:22:16.38191Z",
     "iopub.execute_input": "2022-01-28T00:22:16.382283Z",
     "iopub.status.idle": "2022-01-28T00:22:16.475393Z",
     "shell.execute_reply.started": "2022-01-28T00:22:16.382246Z",
     "shell.execute_reply": "2022-01-28T00:22:16.474317Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5. Evaluate the trained logistic regression model on the test set",
   "metadata": {
    "id": "oKY3G5F66ci_"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import accuracy_score\n\nY_pred = model.predict(X_test) # Make prediction on the test set\nacc = accuracy_score(Y_test, Y_pred) # Compute accuracy score on test set\nprint('Accuracy on test set:', acc)",
   "metadata": {
    "id": "RYHTLYBl6nzI",
    "execution": {
     "iopub.status.busy": "2022-01-28T00:22:55.276644Z",
     "iopub.execute_input": "2022-01-28T00:22:55.27699Z",
     "iopub.status.idle": "2022-01-28T00:22:55.284567Z",
     "shell.execute_reply.started": "2022-01-28T00:22:55.276956Z",
     "shell.execute_reply": "2022-01-28T00:22:55.283488Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6. Visualize the decision boundary\n\nTo visualize the decision boundary of a model on 2d inputs, we need to do the following 3 steps:\n* Create a grid of all points on the 2d input range.\n* Use the model to predict the probability of label 1 for each input.\n* Plot the contours of the predicted probabilities together with the data points.\n\nNow we do **step 1**: creating a grid of all points on the 2d input range.",
   "metadata": {
    "id": "Hza4V8n5T88u"
   }
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\n\nX0 = X[:, 0]\nX1 = X[:, 1]\n\n# Find the range of the 2 dimensions that we will plot\nX0_min, X0_max = X0.min()-1, X0.max()+1\nX1_min, X1_max = X1.min()-1, X1.max()+1\n\nn_steps = 100 # Number of steps on each axis\n\n# Create a meshgrid\nxx, yy = np.meshgrid(np.arange(X0_min, X0_max, (X0_max-X0_min)/n_steps),\n                     np.arange(X1_min, X1_max, (X1_max-X1_min)/n_steps))",
   "metadata": {
    "id": "mJ1PSvHlUDRS",
    "execution": {
     "iopub.status.busy": "2022-01-28T00:25:12.795442Z",
     "iopub.execute_input": "2022-01-28T00:25:12.79634Z",
     "iopub.status.idle": "2022-01-28T00:25:12.804752Z",
     "shell.execute_reply.started": "2022-01-28T00:25:12.796283Z",
     "shell.execute_reply": "2022-01-28T00:25:12.803643Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Step 2:** we predict the model for each point on the meshgrid. Here we use the [predict_proba](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) function to get the probability values. For each example, this function will return the probability for all labels. So the result will be an $n \\times c$ matrix where $n$ is the number of examples and $c$ is the number of labels.",
   "metadata": {
    "id": "sL5d-oy4XbZv"
   }
  },
  {
   "cell_type": "code",
   "source": "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\nZ1 = Z[:, 1] # Here we use the second column of the predictions, which corresponds to the label 1.\nZ1 = Z1.reshape(xx.shape)",
   "metadata": {
    "id": "hd4XpxGfVDoW",
    "execution": {
     "iopub.status.busy": "2022-01-28T00:25:21.426792Z",
     "iopub.execute_input": "2022-01-28T00:25:21.427502Z",
     "iopub.status.idle": "2022-01-28T00:25:21.437966Z",
     "shell.execute_reply.started": "2022-01-28T00:25:21.427446Z",
     "shell.execute_reply": "2022-01-28T00:25:21.436723Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Step 3:** we plot the data and contour of the probability.",
   "metadata": {
    "id": "FkGe31AkYC75"
   }
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\nplt.contourf(xx, yy, Z1, cmap=plt.cm.coolwarm, alpha=0.8)\nplt.scatter(X0, X1, c=Y, cmap=plt.cm.coolwarm, s=50, edgecolors='k')\nplt.xlabel('x0')\nplt.ylabel('x1')\nplt.colorbar()\nplt.show()",
   "metadata": {
    "id": "auRheOCiVI2t",
    "execution": {
     "iopub.status.busy": "2022-01-28T00:26:24.334501Z",
     "iopub.execute_input": "2022-01-28T00:26:24.335027Z",
     "iopub.status.idle": "2022-01-28T00:26:24.629246Z",
     "shell.execute_reply.started": "2022-01-28T00:26:24.334991Z",
     "shell.execute_reply": "2022-01-28T00:26:24.628384Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "\n\n\n\n\n\n\n\n",
   "metadata": {}
  }
 ]
}
