{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bayesian Neural Networks (non-examinable)\n* This demo is based on the following tutorial: https://keras.io/examples/keras_recipes/bayesian_neural_networks/","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup the data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\n\n\ndef get_train_and_test_splits(train_size, batch_size=1):\n    # We prefetch with a buffer the same size as the dataset because th dataset\n    # is very small and fits into memory.\n    dataset = (\n        tfds.load(name=\"wine_quality\", as_supervised=True, split=\"train\")\n        .map(lambda x, y: (x, tf.cast(y, tf.float32)))\n        .prefetch(buffer_size=dataset_size)\n        .cache()\n    )\n    # We shuffle with a buffer the same size as the dataset.\n    train_dataset = (\n        dataset.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)\n    )\n    test_dataset = dataset.skip(train_size).batch(batch_size)\n\n    return train_dataset, test_dataset\n\n\ndataset_size = 4898\nbatch_size = 256\ntrain_size = int(dataset_size * 0.85)\ntrain_dataset, test_dataset = get_train_and_test_splits(train_size, batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Helper method to compile, train, and evaluate the model","metadata":{}},{"cell_type":"code","source":"hidden_units = [8, 8]\nlearning_rate = 0.001\n\n\ndef run_experiment(model, loss, train_dataset, test_dataset):\n\n    model.compile(\n        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n        loss=loss,\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    print(\"Start training the model...\")\n    model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)\n    print(\"Model training finished.\")\n    _, rmse = model.evaluate(train_dataset, verbose=0)\n    print(f\"Train RMSE: {round(rmse, 3)}\")\n\n    print(\"Evaluating model performance...\")\n    _, rmse = model.evaluate(test_dataset, verbose=0)\n    print(f\"Test RMSE: {round(rmse, 3)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Define the model","metadata":{}},{"cell_type":"code","source":"FEATURE_NAMES = [\n    \"fixed acidity\",\n    \"volatile acidity\",\n    \"citric acid\",\n    \"residual sugar\",\n    \"chlorides\",\n    \"free sulfur dioxide\",\n    \"total sulfur dioxide\",\n    \"density\",\n    \"pH\",\n    \"sulphates\",\n    \"alcohol\",\n]\n\n\ndef create_model_inputs():\n    inputs = {}\n    for feature_name in FEATURE_NAMES:\n        inputs[feature_name] = layers.Input(\n            name=feature_name, shape=(1,), dtype=tf.float32\n        )\n    return inputs\n\n\n# Define the prior weight distribution as Normal of mean=0 and stddev=1.\n# Note that, in this example, the we prior distribution is not trainable,\n# as we fix its parameters.\ndef prior(kernel_size, bias_size, dtype=None):\n    n = kernel_size + bias_size\n    prior_model = keras.Sequential(\n        [\n            tfp.layers.DistributionLambda(\n                lambda t: tfp.distributions.MultivariateNormalDiag(\n                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n                )\n            )\n        ]\n    )\n    return prior_model\n\n\n# Define variational posterior weight distribution as multivariate Gaussian.\n# Note that the learnable parameters for this distribution are the means,\n# variances, and covariances.\ndef posterior(kernel_size, bias_size, dtype=None):\n    n = kernel_size + bias_size\n    posterior_model = keras.Sequential(\n        [\n            tfp.layers.VariableLayer(\n                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n            ),\n            tfp.layers.MultivariateNormalTriL(n),\n        ]\n    )\n    return posterior_model\n\n\ndef create_bnn_model(train_size):\n    inputs = create_model_inputs()\n    features = keras.layers.concatenate(list(inputs.values()))\n    features = layers.BatchNormalization()(features)\n\n    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n    for units in hidden_units:\n        features = tfp.layers.DenseVariational(\n            units=units,\n            make_prior_fn=prior,\n            make_posterior_fn=posterior,\n            kl_weight=1 / train_size,\n            activation=\"sigmoid\",\n        )(features)\n\n    # The output is deterministic: a single point estimate.\n    outputs = layers.Dense(units=1)(features)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Experiment with the model on small training set","metadata":{}},{"cell_type":"code","source":"num_epochs = 500\ntrain_sample_size = int(train_size * 0.3)\nsmall_train_dataset = train_dataset.unbatch().take(train_sample_size).batch(batch_size)\n\nbnn_model_small = create_bnn_model(train_sample_size)\nmse_loss = keras.losses.MeanSquaredError()\nrun_experiment(bnn_model_small, mse_loss, small_train_dataset, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Make prediction using posterior","metadata":{}},{"cell_type":"code","source":"def compute_predictions(model, iterations=100):\n    predicted = []\n    for _ in range(iterations):\n        predicted.append(model(examples).numpy())\n    predicted = np.concatenate(predicted, axis=1)\n\n    prediction_mean = np.mean(predicted, axis=1).tolist()\n    prediction_min = np.min(predicted, axis=1).tolist()\n    prediction_max = np.max(predicted, axis=1).tolist()\n    prediction_range = (np.max(predicted, axis=1) - np.min(predicted, axis=1)).tolist()\n\n    for idx in range(sample):\n        print(\n            f\"Predictions mean: {round(prediction_mean[idx], 2)}, \"\n            f\"min: {round(prediction_min[idx], 2)}, \"\n            f\"max: {round(prediction_max[idx], 2)}, \"\n            f\"range: {round(prediction_range[idx], 2)} - \"\n            f\"Actual: {targets[idx]}\"\n        )\n\nsample = 10\nexamples, targets = list(test_dataset.unbatch().shuffle(batch_size * 10).batch(sample))[0]\n\ncompute_predictions(bnn_model_small)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Re-run the experiment with the full training data","metadata":{}},{"cell_type":"code","source":"num_epochs = 500\nbnn_model_full = create_bnn_model(train_size)\nrun_experiment(bnn_model_full, mse_loss, train_dataset, test_dataset)\n\ncompute_predictions(bnn_model_full)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}